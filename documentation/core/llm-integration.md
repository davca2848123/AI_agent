# LLM Integrace

> **Navigace:** [ğŸ“‚ Dokumentace](../README.md) | [ğŸ§  Core](../README.md#core-jÃ¡dro) | [LLM Integrace](llm-integration.md)

> LokÃ¡lnÃ­ LLM pomocÃ­ llama-cpp-python.
> **Verze:** Beta - CLOSED

---

<a name="pÅ™ehled"></a>
## ğŸ“‹ PÅ™ehled

Agent pouÅ¾Ã­vÃ¡ lokÃ¡lnÃ­ LLM model (Qwen 2.5) bÄ›Å¾Ã­cÃ­ pÅ™es `llama-cpp-python` pro rozhodovÃ¡nÃ­ a generovÃ¡nÃ­ odpovÄ›dÃ­.

---

<a name="llmclient-class"></a>
## LLMClient Class

<a name="inicializace"></a>
### ğŸ”§ Inicializace

```python
from agent.llm import LLMClient

llm = LLMClient(
    model_repo="Qwen/Qwen2.5-0.5B-Instruct-GGUF",
    model_filename="qwen2.5-0.5b-instruct-q4_k_m.gguf"
)
```

<a name="model-download"></a>
### ğŸ“¦ Model Download

Model se automaticky stahuje z Hugging Face:

```python
def _verify_model_cache(self):
    """Verify model is downloaded locally."""
    
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    model_path = os.path.join(cache_dir, f"models--{self.model_repo.replace('/', '--')}")
    
    if not os.path.exists(model_path):
        logger.info(f"Model not found in cache. Downloading {self.model_repo}...")
        
        from huggingface_hub import hf_hub_download
        hf_hub_download(
            repo_id=self.model_repo,
            filename=self.model_filename
        )
```

---

<a name="model-loading"></a>
## Model Loading

<a name="_load_model"></a>
### ğŸ”§ _load_model()

```python
def _load_model(self, n_ctx=2048, n_threads=4):
    """Loads the LLM model with dynamic parameters."""
    
    if not Llama:
        logger.error("llama-cpp-python not installed!")
        return
    
    # Get model path from cache
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    # ... find model file ...
    
    self.llm = Llama(
        model_path=model_file,
        n_ctx=n_ctx,          # Context window
        n_threads=n_threads,   # CPU threads
        n_batch=512,          # Batch size
        verbose=False
    )
    
    logger.info(f"LLM loaded: {self.model_filename} (ctx={n_ctx}, threads={n_threads})")
```

<a name="dynamickÃ©-parametry"></a>
### âš™ï¸ DynamickÃ© Parametry

Parametry se mÄ›nÃ­ podle resource tier:

```python
def update_parameters(self, resource_tier: int):
    """Update LLM parameters based on resource tier."""
    
    if resource_tier == 0:  # Normal
        n_ctx, n_threads = 2048, 4
    elif resource_tier == 1:  # Tier 1 (85%)
        n_ctx, n_threads = 1024, 3
    elif resource_tier == 2:  # Tier 2 (90%)
        n_ctx, n_threads = 1024, 2 # VÃ½raznÄ›jÅ¡Ã­ redukce CPU
    else:  # Tier 3 (95%)
        n_ctx, n_threads = 1024, 1 # ZachovÃ¡nÃ­ stability systÃ©mu
    
    # Reload model with new params
    self._load_model(n_ctx, n_threads)
```

---

<a name="generating-responses"></a>

<a name="generovÃ¡nÃ­-odpovÄ›dÃ­"></a>
## GenerovÃ¡nÃ­ OdpovÄ›dÃ­

<a name="generate_response"></a>
### ğŸ”§ generate_response()

```python
response = await llm.generate_response(
    prompt="What is Python?",
    system_prompt="You are a helpful assistant."
)
```

<a name="implementace"></a>
### ğŸ’¡ Implementace

```python
async def generate_response(self, prompt: str, system_prompt: str = "You are an autonomous AI agent."):
    """Generates a response asynchronously."""
    
    if not self.llm:
        return "LLM not available."
    
    # Build full prompt
    full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
    
    # Run in thread pool (blocking operation)
    def run_inference():
        output = self.llm(
            full_prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.9,
            stop=["User:", "\n\n"],
            echo=False
        )
        return output['choices'][0]['text'].strip()
    
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(None, run_inference)
    
    return response
```

<a name="parametry-generovÃ¡nÃ­"></a>
### âš™ï¸ Parametry GenerovÃ¡nÃ­

| Parametr | Hodnota | Popis |
|----------|---------|-------|
| Parametr | Hodnota | Popis |
|----------|---------|-------|
| `max_tokens` | 128 | Max dÃ©lka odpovÄ›di (dynamicky 1024 // 8) |
| `temperature` | N/A | Default (nenÃ­ explicitnÄ› nastaveno) |
| `stop` | `["<|im_end|>"]` | Stop sekvence |

---

<a name="decision-making"></a>

<a name="rozhodovÃ¡nÃ­-o-akcÃ­ch"></a>
## RozhodovÃ¡nÃ­ o AkcÃ­ch

<a name="decide_action"></a>
### ğŸ¯ decide_action()

```python
action = await llm.decide_action(
    context="Boredom: 85%, No recent actions",
    past_memories=memories,
    tools_desc=tools_description
)
```

<a name="implementace"></a>
### ğŸ”§ Implementace

```python
async def decide_action(self, context: str, past_memories=None, tools_desc=None):
    """Decides on an action based on context, memories, and tools."""
    
    # Build system prompt
    system_prompt = """You are an autonomous AI agent. 
You can use tools to interact with the world.
Decide what action to take based on your current state."""
    
    if tools_desc:
        system_prompt += f"\n\nAvailable tools:\n{tools_desc}"
    
    # Build prompt
    prompt = f"Current status:\n{context}\n\n"
    
    if past_memories:
        prompt += "Recent memories:\n"
        for m in past_memories[:5]:
            prompt += f"- {m['content']}\n"
    
    prompt += "\nWhat should I do next? Be specific."
    
    # Generate decision
    decision = await self.generate_response(prompt, system_prompt)
    
    return decision
```

---

<a name="tool-call-parsing"></a>
## Tool Call Parsing

<a name="parse_tool_call"></a>
### ğŸ”§ parse_tool_call()

Extrahuje tool call z LLM odpovÄ›di:

```python
tool_call = llm.parse_tool_call(response)
# Returns: {"tool": "web_tool", "params": {"action": "search", "query": "Python"}}
```

<a name="implementace"></a>
### ğŸ’¡ Implementace

```python
def parse_tool_call(self, response: str):
    """Parses a tool call from the LLM response."""
    
    # Look for TOOL_CALL pattern
    if "TOOL_CALL:" in response:
        lines = response.split('\n')
        tool_name = None
        params = {}
        
        for line in lines:
            if line.startswith("TOOL_CALL:"):
                tool_name = line.split(":", 1)[1].strip()
            elif line.startswith("ARGS:"):
                args_str = line.split(":", 1)[1].strip()
                try:
                    params = json.loads(args_str)
                except:
                    pass
        
        if tool_name:
            return {"tool": tool_name, "params": params}
    
    # Try JSON extraction
    try:
        json_start = response.find('{')
        json_end = response.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = response[json_start:json_end]
            data = json.loads(json_str)
            
            if 'tool' in data:
                return data
    except:
        pass
    
    return None
```

---

<a name="provider-type"></a>
## Provider Type

<a name="provider_type-property"></a>
### ğŸ”§ provider_type Property

```python
@property
def provider_type(self):
    """Returns the type of LLM provider (Local/Cloud)."""
    if self.llm:
        return "Local"
    return "Unavailable"
```

PouÅ¾itÃ­ v `!status`:
```
LLM: âœ… Online (245ms) [Local]
```

---

<a name="error-handling"></a>
## Error Handling

<a name="graceful-degradation"></a>
### âš ï¸ Graceful Degradation

```python
if not self.llm:
    logger.warning("LLM not available!")
    return "LLM not available."
```

Pokud LLM selÅ¾e:
1. VrÃ¡tÃ­ "LLM not available."
2. Agent pouÅ¾ije fallback (napÅ™. web search)
3. Admin je notifikovÃ¡n

---

<a name="performance"></a>
## Performance

<a name="typickÃ©-latence"></a>
### ğŸ“Š TypickÃ© Latence

| Operace | Latence | PoznÃ¡mka |
|---------|---------|----------|
| Simple prompt | 200-300ms | KrÃ¡tkÃ¡ odpovÄ›Ä |
| Decision making | 500-800ms | KomplexnÃ­ prompt |
| Tool selection | 300-500ms | JSON output |
| Long response | 1-2s | Max tokens |

<a name="optimalizace"></a>
### âš¡ Optimalizace

**CPU Threads:**
- VÃ­ce threads = rychlejÅ¡Ã­, ale vÃ­ce CPU
- Default: 4 threads
- Tier 3: 1 thread (maximÃ¡lnÃ­ stabilita)

**Context Window:**
- VÄ›tÅ¡Ã­ window = vÃ­ce pamÄ›ti
- Default: 2048 tokens
- Tier 3: 1024 tokens (zachovÃ¡n kontext)

---

<a name="model-specs"></a>
## Model Specs

<a name="qwen-25-05b-instruct"></a>
### ğŸ“Š Qwen 2.5-0.5B-Instruct

| Vlastnost | Hodnota |
|-----------|---------|
| Parameters | 0.5 billion |
| Quantization | Q4_K_M (4-bit) |
| File Size | ~330 MB |
| RAM Usage | ~500-800 MB |
| Vocabulary | 151,936 tokens |
| Max Context | 32,768 tokens (omezeno na 2048) |

<a name="proÄ-qwen-25"></a>
### ğŸ¯ ProÄ Qwen 2.5?

- **MalÃ½** - BÄ›Å¾Ã­ na RPI
- **RychlÃ½** - 200-500ms latence
- **ChytrÃ½** - DobrÃ© porozumÄ›nÃ­
- **MultijazyÄnÃ½** - ÄŒeÅ¡tina + AngliÄtina
- **InstrukÄnÃ­** - OptimalizovanÃ½ pro pÅ™Ã­kazy

---

<a name="souvisejÃ­cÃ­"></a>
## ğŸ”— SouvisejÃ­cÃ­

- [ğŸ“– Autonomous Behavior](autonomous-behavior.md) - Jak agent pouÅ¾Ã­vÃ¡ LLM
- [`!ask`](../commands/tools-learning.md#ask) - PÅ™Ã­kaz s LLM
- [ğŸ“– Resource Manager](resource-manager.md) - AdaptivnÃ­ parametry LLM
- [ğŸ“š API Reference](../api/llm-integration.md) - TechnickÃ¡ dokumentace tÅ™Ã­d a metod
- [ğŸ—ï¸ Architektura](../architecture.md)


---
PoslednÃ­ aktualizace: 2025-12-06  
Verze: Beta - CLOSED  
Tip: PouÅ¾ij Ctrl+F pro vyhledÃ¡vÃ¡nÃ­
