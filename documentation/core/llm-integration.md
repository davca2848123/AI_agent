# LLM Integrace

> **Navigace:** [üìÇ Dokumentace](../README.md) | [üß† Core](../README.md#core-j√°dro) | [LLM Integrace](llm-integration.md)

> Lok√°ln√≠ LLM + Cloud Gemini Hybrid.
> **Verze:** Beta - CLOSED

---

<a name="p≈ôehled"></a>
## üìã P≈ôehled

Agent pou≈æ√≠v√° hybridn√≠ syst√©m:
1. **Lok√°ln√≠ LLM** (Qwen 2.5 via `llama-cpp-python`) pro rychl√©, soukrom√© rozhodov√°n√≠ a jednoduch√© dotazy.
2. **Cloud LLM** (Google Gemini) pro komplexn√≠ dotazy (`!ask`), obr√°zky a jako **fallback** p≈ôi v√Ωpadku lok√°ln√≠ho modelu.

---

<a name="llmclient-class"></a>
## LLMClient Class

<a name="inicializace"></a>
### üîß Inicializace

```python
from agent.llm import LLMClient

llm = LLMClient(
    model_repo="Qwen/Qwen2.5-0.5B-Instruct-GGUF",
    model_filename="qwen2.5-0.5b-instruct-q4_k_m.gguf"
)
```

<a name="model-download"></a>
### üì¶ Model Download

Model se automaticky stahuje z Hugging Face:

```python
def _verify_model_cache(self):
    """Verify model is downloaded locally."""
    
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    model_path = os.path.join(cache_dir, f"models--{self.model_repo.replace('/', '--')}")
    
    if not os.path.exists(model_path):
        logger.info(f"Model not found in cache. Downloading {self.model_repo}...")
        
        from huggingface_hub import hf_hub_download
        hf_hub_download(
            repo_id=self.model_repo,
            filename=self.model_filename
        )
```

---

<a name="model-loading"></a>
## Model Loading

<a name="_load_model"></a>
### üîß _load_model()

```python
def _load_model(self, n_ctx=2048, n_threads=4):
    """Loads the LLM model with dynamic parameters."""
    
    if not Llama:
        logger.error("llama-cpp-python not installed!")
        return
    
    # Get model path from cache
    cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
    # ... find model file ...
    
    self.llm = Llama(
        model_path=model_file,
        n_ctx=n_ctx,          # Context window
        n_threads=n_threads,   # CPU threads
        n_batch=512,          # Batch size
        verbose=False
    )
    
    logger.info(f"LLM loaded: {self.model_filename} (ctx={n_ctx}, threads={n_threads})")
```

<a name="dynamick√©-parametry"></a>
### ‚öôÔ∏è Dynamick√© Parametry

Parametry se mƒõn√≠ podle resource tier:

```python
def update_parameters(self, resource_tier: int):
    """Update LLM parameters based on resource tier."""
    
    if resource_tier == 0:  # Normal
        n_ctx, n_threads = 2048, 4
    elif resource_tier == 1:  # Tier 1 (85%)
        n_ctx, n_threads = 1024, 3
    elif resource_tier == 2:  # Tier 2 (90%)
        n_ctx, n_threads = 1024, 2 # V√Ωraznƒõj≈°√≠ redukce CPU
    else:  # Tier 3 (95%)
        n_ctx, n_threads = 1024, 1 # Zachov√°n√≠ stability syst√©mu
    
    # Reload model with new params
    self._load_model(n_ctx, n_threads)
```

---

<a name="generating-responses"></a>

<a name="generov√°n√≠-odpovƒõd√≠"></a>
## Generov√°n√≠ Odpovƒõd√≠

<a name="generate_response"></a>
### üîß generate_response()

```python
response = await llm.generate_response(
    prompt="What is Python?",
    system_prompt="You are a helpful assistant."
)
```

<a name="implementace"></a>
### üí° Implementace

```python
async def generate_response(self, prompt: str, system_prompt: str = "You are an autonomous AI agent."):
    """Generates a response asynchronously."""
    
    if not self.llm:
        return "LLM not available."
    
    # Build full prompt
    full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
    
    # Run in thread pool (blocking operation)
    def run_inference():
        output = self.llm(
            full_prompt,
            max_tokens=256,
            temperature=0.7,
            top_p=0.9,
            stop=["User:", "\n\n"],
            echo=False
        )
        return output['choices'][0]['text'].strip()
    
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(None, run_inference)
    
    return response
```

<a name="parametry-generov√°n√≠"></a>
### ‚öôÔ∏è Parametry Generov√°n√≠

| Parametr | Hodnota | Popis |
|----------|---------|-------|
| Parametr | Hodnota | Popis |
|----------|---------|-------|
| `max_tokens` | 128 | Max d√©lka odpovƒõdi (dynamicky 1024 // 8) |
| `temperature` | N/A | Default (nen√≠ explicitnƒõ nastaveno) |
| `stop` | `["<|im_end|>"]` | Stop sekvence |

---

<a name="decision-making"></a>

<a name="rozhodov√°n√≠-o-akc√≠ch"></a>
## Rozhodov√°n√≠ o Akc√≠ch

<a name="decide_action"></a>
### üéØ decide_action()

```python
action = await llm.decide_action(
    context="Boredom: 85%, No recent actions",
    past_memories=memories,
    tools_desc=tools_description
)
```

<a name="implementace"></a>
### üîß Implementace

```python
async def decide_action(self, context: str, past_memories=None, tools_desc=None):
    """Decides on an action based on context, memories, and tools."""
    
    # Build system prompt
    system_prompt = """You are an autonomous AI agent. 
You can use tools to interact with the world.
Decide what action to take based on your current state."""
    
    if tools_desc:
        system_prompt += f"\n\nAvailable tools:\n{tools_desc}"
    
    # Build prompt
    prompt = f"Current status:\n{context}\n\n"
    
    if past_memories:
        prompt += "Recent memories:\n"
        for m in past_memories[:5]:
            prompt += f"- {m['content']}\n"
    
    prompt += "\nWhat should I do next? Be specific."
    
    # Generate decision
    decision = await self.generate_response(prompt, system_prompt)
    
    return decision
```

---

<a name="tool-call-parsing"></a>
## Tool Call Parsing

<a name="parse_tool_call"></a>
### üîß parse_tool_call()

Extrahuje tool call z LLM odpovƒõdi:

```python
tool_call = llm.parse_tool_call(response)
# Returns: {"tool": "web_tool", "params": {"action": "search", "query": "Python"}}
```

<a name="implementace"></a>
### üí° Implementace

```python
def parse_tool_call(self, response: str):
    """Parses a tool call from the LLM response."""
    
    # Look for TOOL_CALL pattern
    if "TOOL_CALL:" in response:
        lines = response.split('\n')
        tool_name = None
        params = {}
        
        for line in lines:
            if line.startswith("TOOL_CALL:"):
                tool_name = line.split(":", 1)[1].strip()
            elif line.startswith("ARGS:"):
                args_str = line.split(":", 1)[1].strip()
                try:
                    params = json.loads(args_str)
                except:
                    pass
        
        if tool_name:
            return {"tool": tool_name, "params": params}
    
    # Try JSON extraction
    try:
        json_start = response.find('{')
        json_end = response.rfind('}') + 1
        
        if json_start != -1 and json_end > json_start:
            json_str = response[json_start:json_end]
            data = json.loads(json_str)
            
            if 'tool' in data:
                return data
    except:
        pass
    
    return None
```

---

<a name="provider-type"></a>
## Provider Type

<a name="provider_type-property"></a>
### üîß provider_type Property

```python
@property
def provider_type(self):
    """Returns the type of LLM provider (Local/Cloud)."""
    if self.llm:
        return "Local"
    return "Unavailable"
```

Pou≈æit√≠ v `!status`:
```
LLM: ‚úÖ Online (245ms) [Local]
```

---

<a name="error-handling"></a>
## Error Handling

<a name="graceful-degradation"></a>
### ‚ö†Ô∏è Graceful Degradation

```python
if not self.llm:
    logger.warning("LLM not available!")
    return "LLM not available."
```

Pokud LLM sel≈æe:
1. Vr√°t√≠ "LLM not available."
2. Agent pou≈æije fallback (nap≈ô. web search)
3. Admin je notifikov√°n

---

<a name="performance"></a>
## Performance

<a name="typick√©-latence"></a>
### üìä Typick√© Latence

| Operace | Latence | Pozn√°mka |
|---------|---------|----------|
| Simple prompt | 200-300ms | Kr√°tk√° odpovƒõƒè |
| Decision making | 500-800ms | Komplexn√≠ prompt |
| Tool selection | 300-500ms | JSON output |
| Long response | 1-2s | Max tokens |

<a name="optimalizace"></a>
### ‚ö° Optimalizace

**CPU Threads:**
- V√≠ce threads = rychlej≈°√≠, ale v√≠ce CPU
- Default: 4 threads
- Tier 3: 1 thread (maxim√°ln√≠ stabilita)

**Context Window:**
- Vƒõt≈°√≠ window = v√≠ce pamƒõti
- Default: 2048 tokens
- Tier 3: 1024 tokens (zachov√°n kontext)

---

<a name="model-specs"></a>
## Model Specs

<a name="qwen-25-05b-instruct"></a>
### üìä Qwen 2.5-0.5B-Instruct

| Vlastnost | Hodnota |
|-----------|---------|
| Parameters | 0.5 billion |
| Quantization | Q4_K_M (4-bit) |
| File Size | ~330 MB |
| RAM Usage | ~500-800 MB |
| Vocabulary | 151,936 tokens |
| Max Context | 32,768 tokens (omezeno na 2048) |

<a name="proƒç-qwen-25"></a>
### üéØ Proƒç Qwen 2.5?

- **Mal√Ω** - Bƒõ≈æ√≠ na RPI
- **Rychl√Ω** - 200-500ms latence
- **Chytr√Ω** - Dobr√© porozumƒõn√≠
- **Multijazyƒçn√Ω** - ƒåe≈°tina + Angliƒçtina
- **Instrukƒçn√≠** - Optimalizovan√Ω pro p≈ô√≠kazy

---

<a name="cloud-integration"></a>
## ‚òÅÔ∏è Cloud Integration (Gemini)

Agent implementuje hybridn√≠ model, kde kombinuje lok√°ln√≠ LLM s cloudov√Ωm Google Gemini API pro zv√Ω≈°en√≠ robustnosti a schopnost√≠.

<a name="konfigurace"></a>
### üîß Konfigurace
Modely jsou konfigurov√°ny v `config_settings.py` (s mapov√°n√≠m na `latest` verze):
*   **Fast Model** (`gemini-flash-latest`): Pro rychl√© odpovƒõdi a fallback.
*   **High Model** (`gemini-pro-latest`): Pro komplexn√≠ anal√Ωzu a `!ask` s obr√°zky.

<a name="smart-routing"></a>
### üß† Smart Routing (`!ask`)
P≈ô√≠kaz `!ask` automaticky vol√≠ nejvhodnƒõj≈°√≠ model:
1.  **Local LLM**: Pro jednoduch√© dotazy (< 50 znak≈Ø).
2.  **Gemini (High)**: Pro:
    *   Dlouh√©/komplexn√≠ dotazy (> 50 znak≈Ø)
    *   Dotazy s obr√°zky (Vision capabilities)
    *   Kdy≈æ u≈æivatel explicitnƒõ po≈æ√°d√°

<a name="autonomous-fallback"></a>
### üõ°Ô∏è Autonomous Fallback
Pro zaji≈°tƒõn√≠ nep≈ôetr≈æit√©ho provozu i p≈ôi v√Ωpadku lok√°ln√≠ho modelu (nap≈ô. chybƒõj√≠c√≠ binaries, p≈ôet√≠≈æen√≠):
1.  Pokud `decide_action` (lok√°ln√≠ LLM) sel≈æe (`LLM not available`), agent zachyt√≠ chybu.
2.  Automaticky p≈ôepne na **Gemini (Fast)** pro rozhodovac√≠ proces.
3.  Tato z√°loha umo≈æ≈àuje agentovi pokraƒçovat v autonomn√≠ ƒçinnosti ("Thinking...") i bez lok√°ln√≠ho mozku.

<a name="api-metody"></a>
### üíª API Metody
Pou≈æit√≠ v k√≥du (`LLMClient`):

```python
response = await llm.ask_gemini(
    prompt="Describe this image",
    image_data=bytes_data,
    model_type="high" # 'high' or 'fast'
)
```

---

<a name="souvisej√≠c√≠"></a>
## üîó Souvisej√≠c√≠

- [üìñ Autonomous Behavior](autonomous-behavior.md) - Jak agent pou≈æ√≠v√° LLM
- [`!ask`](../commands/tools-learning.md#ask) - P≈ô√≠kaz s LLM
- [üìñ Resource Manager](resource-manager.md) - Adaptivn√≠ parametry LLM
- [üìö API Reference](../api/llm-integration.md) - Technick√° dokumentace t≈ô√≠d a metod
- [üèóÔ∏è Architektura](../architecture.md)
---
Posledn√≠ aktualizace: 2025-12-13  
Verze: Beta - CLOSED  
Tip: Pou≈æij Ctrl+F pro vyhled√°v√°n√≠
